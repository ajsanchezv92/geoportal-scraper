#!/usr/bin/env python3
"""
SCRIPT PRINCIPAL DE INICIO - GeoScrape Sentinel
Sistema completo de scraping resiliente para Geoportal Minetur

Caracter√≠sticas:
‚úÖ Inicia todos los componentes del sistema
‚úÖ Detecta y reanuda desde checkpoints autom√°ticamente  
‚úÖ Manejo elegante de paradas (Ctrl+C)
‚úÖ Supervivencia a reinicios y cierres
‚úÖ Nunca pierde el progreso
"""

import asyncio
import signal
import sys
import time
from pathlib import Path

# Agregar src al path para imports
src_path = Path(__file__).parent.parent / 'src'
sys.path.append(str(src_path))

try:
    from scraper_principal import GeoportalScraper, ScraperConfig
    from guardado_automatico import SistemaGuardado
    from sesiones_automaticas import GestorSesiones
    from url_manager import URLManager
    from config_manager import ConfigManager
    print("‚úÖ Todos los m√≥dulos importados correctamente")
except ImportError as e:
    print(f"‚ùå Error importando m√≥dulos: {e}")
    print("üí° Aseg√∫rate de que todos los archivos est√©n en la estructura correcta")
    sys.exit(1)


class IniciadorSentinel:
    """Clase principal que orchesta todo el sistema de scraping"""
    
    def __init__(self):
        self.scraper = None
        self.guardado = None
        self.sesiones = None
        self.url_manager = None
        self.config_manager = ConfigManager()
        self.ejecucion_activa = True
        
    async def inicializar_sistema(self):
        """Inicializa todos los componentes del sistema"""
        print("\n" + "="*60)
        print("üõ°Ô∏è  INICIANDO GEOSCRAPE SENTINEL")
        print("="*60)
        
        # Cargar configuraci√≥n
        config_data = self.config_manager.load_config()
        scraper_config_data = config_data.get('scraper', {})
        
        # Crear configuraci√≥n del scraper
        config = ScraperConfig(
            max_workers=scraper_config_data.get('max_workers', 8),
            batch_size=scraper_config_data.get('batch_size', 25),
            timeout=scraper_config_data.get('timeout', 25),
            checkpoint_interval=scraper_config_data.get('checkpoint_interval', 3),
            max_retries=scraper_config_data.get('max_retries', 3),
            retry_delay=scraper_config_data.get('retry_delay', 2),
            request_delay=scraper_config_data.get('request_delay', 0.1),
            random_delay=scraper_config_data.get('random_delay', True),
            connection_pool_size=scraper_config_data.get('connection_pool_size', 12),
            progress_update_interval=scraper_config_data.get('progress_update_interval', 50),
            memory_check_interval=scraper_config_data.get('memory_check_interval', 25)
        )
        
        # Inicializar componentes
        self.scraper = GeoportalScraper(config)
        self.guardado = SistemaGuardado()
        self.sesiones = GestorSesiones()
        self.url_manager = URLManager()
        
        print("‚úÖ Sistema inicializado correctamente")
        return True
    
    async def cargar_urls(self):
        """Carga las URLs desde Google Drive"""
        print("\nüì• CARGANDO URLs DESDE GOOGLE DRIVE...")
        
        # URL del archivo CSV en Google Drive
        drive_url = "https://drive.google.com/file/d/1jcKPQHXLo1hbwAd2ucg60qmn66P1s8P6/view?usp=drive_link"
        
        urls = await self.url_manager.cargar_urls_desde_drive(drive_url)
        
        if not urls:
            print("‚ùå No se pudieron cargar las URLs desde Google Drive")
            return False
        
        print(f"‚úÖ {len(urls)} URLs cargadas correctamente")
        return urls
    
    async verificar_checkpoints(self):
        """Verifica y carga checkpoints existentes"""
        print("\nüîç BUSCANDO CHECKPOINTS ANTERIORES...")
        
        checkpoint_files = list(Path('data/checkpoints').glob('*.json'))
        if checkpoint_files:
            # Encontrar el checkpoint m√°s reciente
            latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            print(f"‚úÖ Checkpoint encontrado: {latest_checkpoint.name}")
            print("üîÑ El sistema reanudar√° desde el √∫ltimo estado guardado")
            return True
        else:
            print("‚ÑπÔ∏è  No se encontraron checkpoints anteriores")
            print("üöÄ Iniciando nueva ejecuci√≥n desde cero")
            return False
    
    async def iniciar_servicios_secundarios(self):
        """Inicia los servicios de guardado y sesiones en segundo plano"""
        print("\nüîÑ INICIANDO SERVICIOS EN SEGUNDO PLANO...")
        
        # Iniciar sistema de guardado autom√°tico
        await self.guardado.iniciar()
        print("‚úÖ Sistema de guardado autom√°tico iniciado")
        
        # Iniciar gestor de sesiones autom√°ticas
        await self.sesiones.iniciar()
        print("‚úÖ Gestor de sesiones autom√°ticas iniciado")
        
        print("üí° Servicios secundarios activos y monitoreando")
    
    async def ejecutar_scraping_principal(self, urls):
        """Ejecuta el scraping principal"""
        print("\nüéØ INICIANDO SCRAPING PRINCIPAL...")
        
        # Filtrar URLs pendientes
        urls_pendientes = self.url_manager.filtrar_urls_pendientes()
        
        if not urls_pendientes:
            print("‚úÖ No hay URLs pendientes - scraping completado!")
            return True
        
        estadisticas = self.url_manager.get_estadisticas_urls()
        print(f"üìä ESTAD√çSTICAS INICIALES:")
        print(f"   ‚Ä¢ URLs totales: {estadisticas['total_urls']}")
        print(f"   ‚Ä¢ URLs procesadas: {estadisticas['procesadas']}")
        print(f"   ‚Ä¢ URLs pendientes: {estadisticas['pendientes']}")
        print(f"   ‚Ä¢ Progreso: {estadisticas['porcentaje_completado']:.1f}%")
        
        # Calcular tiempo estimado (asumiendo ~25 URLs/minuto)
        tiempo_estimado_minutos = estadisticas['pendientes'] / 25
        horas = int(tiempo_estimado_minutos // 60)
        minutos = int(tiempo_estimado_minutos % 60)
        
        print(f"‚è±Ô∏è  TIEMPO ESTIMADO: {horas}h {minutos}m")
        print(f"üöÄ INICIANDO CON {self.scraper.config.max_workers} WORKERS...")
        
        # Ejecutar scraping
        await self.scraper.ejecutar_scraping(urls_pendientes)
        
        return True
    
    def configurar_manejo_se√±ales(self):
        """Configura el manejo elegante de se√±ales (Ctrl+C)"""
        def manejar_se√±al(sig, frame):
            print(f"\nüõë Se√±al {sig} recibida - Iniciando parada elegante...")
            self.ejecucion_activa = False
            asyncio.create_task(self.parada_elegante())
        
        # Registrar manejadores de se√±ales
        signal.signal(signal.SIGINT, manejar_se√±al)   # Ctrl+C
        signal.signal(signal.SIGTERM, manejar_se√±al)  # Terminaci√≥n
        print("‚úÖ Manejadores de se√±ales configurados (Ctrl+C para parada elegante)")
    
    async def parada_elegante(self):
        """Realiza una parada elegante de todo el sistema"""
        print("\n" + "="*50)
        print("üõë INICIANDO PARADA ELEGANTE")
        print("="*50)
        
        # Detener componentes en orden
        if self.scraper:
            print("‚è∏Ô∏è  Deteniendo scraper principal...")
            self.scraper.parada_elegante()
        
        if self.guardado:
            print("üíæ Deteniendo sistema de guardado...")
            await self.guardado.detener()
        
        if self.sesiones:
            print("üîí Deteniendo gestor de sesiones...")
            await self.sesiones.detener()
        
        print("‚úÖ Parada elegante completada")
        print("üìÅ El progreso ha sido guardado y puede reanudarse posteriormente")
    
    async def ejecutar(self):
        """M√©todo principal de ejecuci√≥n"""
        try:
            # 1. Inicializar sistema
            if not await self.inicializar_sistema():
                return False
            
            # 2. Configurar manejo de se√±ales
            self.configurar_manejo_se√±ales()
            
            # 3. Verificar checkpoints existentes
            await self.verificar_checkpoints()
            
            # 4. Cargar URLs
            urls = await self.cargar_urls()
            if not urls:
                return False
            
            # 5. Iniciar servicios secundarios
            await self.iniciar_servicios_secundarios()
            
            # 6. Ejecutar scraping principal
            await self.ejecutar_scraping_principal(urls)
            
            # 7. Parada final elegante
            await self.parada_elegante()
            
            print("\n" + "="*50)
            print("üéâ SCRAPING COMPLETADO EXITOSAMENTE!")
            print("="*50)
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERROR CR√çTICO: {e}")
            print("üí° Intentando parada de emergencia...")
            await self.parada_elegante()
            return False


async def main():
    """Funci√≥n principal"""
    iniciador = IniciadorSentinel()
    exito = await iniciador.ejecutar()
    
    if exito:
        print("\n‚úÖ GeoScrape Sentinel finalizado correctamente")
        sys.exit(0)
    else:
        print("\n‚ùå GeoScrape Sentinel encontr√≥ errores")
        sys.exit(1)


if __name__ == "__main__":
    # Verificar que existe la estructura de directorios
    directorios_necesarios = ['data/checkpoints', 'data/resultados', 'data/logs', 'data/backups', 'config']
    for directorio in directorios_necesarios:
        Path(directorio).mkdir(parents=True, exist_ok=True)
    
    # Ejecutar sistema
    asyncio.run(main())
