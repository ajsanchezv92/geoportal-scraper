#!/usr/bin/env python3
"""
SCRIPT PRINCIPAL DE INICIO - GeoScrape Sentinel
Sistema completo de scraping resiliente para Geoportal Minetur
"""

import asyncio
import signal
import sys
import time
from pathlib import Path

# Agregar src al path para imports
src_path = Path(__file__).parent.parent / 'src'
sys.path.append(str(src_path))

try:
    from scraper_principal import GeoportalScraper, ScraperConfig
    from guardado_automatico import SistemaGuardado
    from sesiones_automaticas import GestorSesiones
    from url_manager import URLManager
    from config_manager import ConfigManager
    print("âœ… Todos los mÃ³dulos importados correctamente")
except ImportError as e:
    print(f"âŒ Error importando mÃ³dulos: {e}")
    print("ğŸ’¡ AsegÃºrate de que todos los archivos estÃ©n en la estructura correcta")
    sys.exit(1)


class IniciadorSentinel:
    """Clase principal que orchesta todo el sistema de scraping"""
    
    def __init__(self):
        self.scraper = None
        self.guardado = None
        self.sesiones = None
        self.url_manager = None
        self.config_manager = ConfigManager()
        self.ejecucion_activa = True
        
    async def inicializar_sistema(self):
        """Inicializa todos los componentes del sistema"""
        print("\n" + "="*60)
        print("ğŸ›¡ï¸  INICIANDO GEOSCRAPE SENTINEL")
        print("="*60)
        
        # Cargar configuraciÃ³n
        config_data = self.config_manager.load_config()
        scraper_config_data = config_data.get('scraper', {})
        
        # Crear configuraciÃ³n del scraper
        config = ScraperConfig(
            max_workers=scraper_config_data.get('max_workers', 8),
            batch_size=scraper_config_data.get('batch_size', 25),
            timeout=scraper_config_data.get('timeout', 25),
            checkpoint_interval=scraper_config_data.get('checkpoint_interval', 3),
            max_retries=scraper_config_data.get('max_retries', 3),
            retry_delay=scraper_config_data.get('retry_delay', 2),
            request_delay=scraper_config_data.get('request_delay', 0.1),
            random_delay=scraper_config_data.get('random_delay', True),
            connection_pool_size=scraper_config_data.get('connection_pool_size', 12),
            progress_update_interval=scraper_config_data.get('progress_update_interval', 50),
            memory_check_interval=scraper_config_data.get('memory_check_interval', 25)
        )
        
        # Inicializar componentes
        self.scraper = GeoportalScraper(config)
        self.guardado = SistemaGuardado()
        self.sesiones = GestorSesiones()
        self.url_manager = URLManager()
        
        print("âœ… Sistema inicializado correctamente")
        return True
    
    async def cargar_urls(self):
        """Carga las URLs desde Google Drive"""
        print("\nğŸ“¥ CARGANDO URLs DESDE GOOGLE DRIVE...")
        
        # URL del archivo CSV en Google Drive
        drive_url = "https://drive.google.com/file/d/1jcKPQHXLo1hbwAd2ucg60qmn66P1s8P6/view?usp=drive_link"
        
        urls = await self.url_manager.cargar_urls_desde_drive(drive_url)
        
        if not urls:
            print("âŒ No se pudieron cargar las URLs desde Google Drive")
            return False
        
        print(f"âœ… {len(urls)} URLs cargadas correctamente")
        return urls
    
    async def verificar_checkpoints(self):
        """Verifica y carga checkpoints existentes"""
        print("\nğŸ” BUSCANDO CHECKPOINTS ANTERIORES...")
        
        checkpoint_files = list(Path('data/checkpoints').glob('*.json'))
        if checkpoint_files:
            # Encontrar el checkpoint mÃ¡s reciente
            latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            print(f"âœ… Checkpoint encontrado: {latest_checkpoint.name}")
            print("ğŸ”„ El sistema reanudarÃ¡ desde el Ãºltimo estado guardado")
            return True
        else:
            print("â„¹ï¸  No se encontraron checkpoints anteriores")
            print("ğŸš€ Iniciando nueva ejecuciÃ³n desde cero")
            return False
    
    async def iniciar_servicios_secundarios(self):
        """Inicia los servicios de guardado y sesiones en segundo plano"""
        print("\nğŸ”„ INICIANDO SERVICIOS EN SEGUNDO PLANO...")
        
        # Iniciar sistema de guardado automÃ¡tico
        await self.guardado.iniciar()
        print("âœ… Sistema de guardado automÃ¡tico iniciado")
        
        # Iniciar gestor de sesiones automÃ¡ticas
        await self.sesiones.iniciar()
        print("âœ… Gestor de sesiones automÃ¡ticas iniciado")
        
        print("ğŸ’¡ Servicios secundarios activos y monitoreando")
    
    async def ejecutar_scraping_principal(self, urls):
        """Ejecuta el scraping principal"""
        print("\nğŸ¯ INICIANDO SCRAPING PRINCIPAL...")
        
        # Filtrar URLs pendientes
        urls_pendientes = self.url_manager.filtrar_urls_pendientes()
        
        if not urls_pendientes:
            print("âœ… No hay URLs pendientes - scraping completado!")
            return True
        
        estadisticas = self.url_manager.get_estadisticas_urls()
        print(f"ğŸ“Š ESTADÃSTICAS INICIALES:")
        print(f"   â€¢ URLs totales: {estadisticas['total_urls']}")
        print(f"   â€¢ URLs procesadas: {estadisticas['procesadas']}")
        print(f"   â€¢ URLs pendientes: {estadisticas['pendientes']}")
        print(f"   â€¢ Progreso: {estadisticas['porcentaje_completado']:.1f}%")
        
        # Calcular tiempo estimado (asumiendo ~25 URLs/minuto)
        tiempo_estimado_minutos = estadisticas['pendientes'] / 25
        horas = int(tiempo_estimado_minutos // 60)
        minutos = int(tiempo_estimado_minutos % 60)
        
        print(f"â±ï¸  TIEMPO ESTIMADO: {horas}h {minutos}m")
        print(f"ğŸš€ INICIANDO CON {self.scraper.config.max_workers} WORKERS...")
        
        # IMPORTANTE: Ejecutar scraping de forma asÃ­ncrona
        try:
            await self.scraper.ejecutar_scraping(urls_pendientes)
            return True
        except Exception as e:
            print(f"âŒ Error en scraping principal: {e}")
            return False
    
    def configurar_manejo_seÃ±ales(self):
        """Configura el manejo elegante de seÃ±ales (Ctrl+C)"""
        def manejar_seÃ±al(sig, frame):
            print(f"\nğŸ›‘ SeÃ±al {sig} recibida - Iniciando parada elegante...")
            self.ejecucion_activa = False
            asyncio.create_task(self.parada_elegante())
        
        # Registrar manejadores de seÃ±ales
        signal.signal(signal.SIGINT, manejar_seÃ±al)   # Ctrl+C
        signal.signal(signal.SIGTERM, manejar_seÃ±al)  # TerminaciÃ³n
        print("âœ… Manejadores de seÃ±ales configurados (Ctrl+C para parada elegante)")
    
    async def parada_elegante(self):
        """Realiza una parada elegante de todo el sistema"""
        print("\n" + "="*50)
        print("ğŸ›‘ INICIANDO PARADA ELEGANTE")
        print("="*50)
        
        # Detener componentes en orden
        if self.scraper:
            print("â¸ï¸  Deteniendo scraper principal...")
            self.scraper.parada_elegante()
        
        if self.guardado:
            print("ğŸ’¾ Deteniendo sistema de guardado...")
            await self.guardado.detener()
        
        if self.sesiones:
            print("ğŸ”’ Deteniendo gestor de sesiones...")
            await self.sesiones.detener()
        
        print("âœ… Parada elegante completada")
        print("ğŸ“ El progreso ha sido guardado y puede reanudarse posteriormente")
    
    async def ejecutar(self):
        """MÃ©todo principal de ejecuciÃ³n"""
        try:
            print("ğŸ”„ PASO 1: Inicializando sistema...")
            # 1. Inicializar sistema
            if not await self.inicializar_sistema():
                return False
            
            print("ğŸ”„ PASO 2: Configurando manejo de seÃ±ales...")
            # 2. Configurar manejo de seÃ±ales
            self.configurar_manejo_seÃ±ales()
            
            print("ğŸ”„ PASO 3: Verificando checkpoints...")
            # 3. Verificar checkpoints existentes
            await self.verificar_checkpoints()
            
            print("ğŸ”„ PASO 4: Cargando URLs...")
            # 4. Cargar URLs
            urls = await self.cargar_urls()
            if not urls:
                print("âŒ No se pudieron cargar las URLs")
                return False
            print(f"âœ… URLs cargadas: {len(urls)}")
            
            print("ğŸ”„ PASO 5: Iniciando servicios secundarios...")
            # 5. Iniciar servicios secundarios
            await self.iniciar_servicios_secundarios()
            
            print("ğŸ”„ PASO 6: Ejecutando scraping principal...")
            # 6. Ejecutar scraping principal
            resultado_scraping = await self.ejecutar_scraping_principal(urls)
            if not resultado_scraping:
                print("âŒ El scraping principal fallÃ³")
                return False
            
            print("ğŸ”„ PASO 7: Parada final elegante...")
            # 7. Parada final elegante
            await self.parada_elegante()
            
            print("\n" + "="*50)
            print("ğŸ‰ SCRAPING COMPLETADO EXITOSAMENTE!")
            print("="*50)
            return True
            
        except Exception as e:
            print(f"\nâŒ ERROR CRÃTICO: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ’¡ Intentando parada de emergencia...")
            await self.parada_elegante()
            return False


async def main():
    """FunciÃ³n principal"""
    print("ğŸš€ INICIANDO GEOSCRAPE SENTINEL...")
    
    iniciador = IniciadorSentinel()
    exito = await iniciador.ejecutar()
    
    if exito:
        print("\nâœ… GeoScrape Sentinel finalizado correctamente")
        sys.exit(0)
    else:
        print("\nâŒ GeoScrape Sentinel encontrÃ³ errores")
        sys.exit(1)


if __name__ == "__main__":
    # Verificar que existe la estructura de directorios
    directorios_necesarios = ['data/checkpoints', 'data/resultados', 'data/logs', 'data/backups', 'config']
    for directorio in directorios_necesarios:
        Path(directorio).mkdir(parents=True, exist_ok=True)
    
    # Ejecutar sistema
    asyncio.run(main())
