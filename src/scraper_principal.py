import asyncio
import aiohttp
import json
import time
import random
import signal
import sys
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass
import logging
import psutil


@dataclass
class ScraperConfig:
    max_workers: int = 8
    batch_size: int = 25
    timeout: int = 25
    checkpoint_interval: int = 3
    max_retries: int = 3
    retry_delay: int = 2
    request_delay: float = 0.1
    random_delay: bool = True
    connection_pool_size: int = 12
    progress_update_interval: int = 50
    memory_check_interval: int = 25


class GeoportalScraper:
    def __init__(self, config: ScraperConfig):
        self.config = config
        self.activo = True
        self.session = None
        self.stats = {
            'urls_procesadas': 0,
            'urls_exitosas': 0,
            'urls_fallidas': 0,
            'inicio_tiempo': time.time(),
            'emplazamientos_validos': 0,
            'urls_procesadas_list': []
        }
        self.setup_logging()
        self.setup_directories()
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('data/logs/scraper.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_directories(self):
        directories = ['data/checkpoints', 'data/resultados', 'data/logs', 'data/backups']
        for dir_path in directories:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    async def _configure_session(self):
        connector = aiohttp.TCPConnector(
            limit=self.config.connection_pool_size,
            limit_per_host=self.config.connection_pool_size
        )
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        )
    
    async def procesar_url_con_delay(self, url: str) -> Optional[Dict]:
        if not self.activo:
            return None
            
        # Delay inteligente
        if self.stats['urls_procesadas'] % 5 == 0:
            delay = self.config.request_delay
            if self.config.random_delay:
                delay += random.uniform(-0.05, 0.05)
            await asyncio.sleep(delay)
        
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    datos = self.extraer_datos_estacion(html, url)
                    self.stats['urls_exitosas'] += 1
                    return datos
                else:
                    self.logger.warning(f"HTTP {response.status} para {url}")
                    self.stats['urls_fallidas'] += 1
                    return None
        except Exception as e:
            self.logger.error(f"Error procesando {url}: {str(e)}")
            self.stats['urls_fallidas'] += 1
            return None
    
    def extraer_datos_estacion(self, html: str, url: str) -> Dict:
        """Extrae datos de la estaci√≥n basado en el ejemplo JSON proporcionado"""
        estacion_id = self._extraer_estacion_id(url)
        
        datos = {
            "estacion_id": estacion_id,
            "url_oficial": url,
            "metadata": self._generar_metadata(),
            "informacion_geografica": self._extraer_info_geografica(html, estacion_id),
            "caracteristicas_estacion": self._extraer_caracteristicas(html),
            "infraestructura_tecnologica": self._extraer_infraestructura(html),
            "mediciones_emisiones": self._extraer_mediciones(html),
            "evaluacion_riesgo_salud": self._evaluar_riesgos(html),
            "analisis_cobertura": self._analizar_cobertura(html),
            "impacto_territorial": self._analizar_impacto(html),
            "estado_actualizacion": self._obtener_estado_actualizacion(html),
            "scraping_metadata": self._generar_metadata_scraping(url)
        }
        
        self.stats['emplazamientos_validos'] += 1
        return datos
    
    def _extraer_estacion_id(self, url: str) -> str:
        """Extrae el ID de estaci√≥n de la URL"""
        try:
            return url.split('emplazamiento=')[1]
        except:
            return "desconocido"
    
    def _generar_metadata(self) -> Dict:
        return {
            "fecha_extraccion": time.strftime('%Y-%m-%dT%H:%M:%S'),
            "fecha_procesamiento": time.strftime('%Y-%m-%dT%H:%M:%S'),
            "version_esquema": "3.0.0",
            "fuente_verificada": True,
            "hash_verificacion": f"hash_{int(time.time())}"
        }
    
    def _extraer_info_geografica(self, html: str, estacion_id: str) -> Dict:
        """Simula extracci√≥n de informaci√≥n geogr√°fica"""
        # En una implementaci√≥n real, aqu√≠ se parsear√≠a el HTML
        return {
            "direccion": {
                "via": f"DIRECCI√ìN PARA {estacion_id}",
                "municipio": f"MUNICIPIO_{estacion_id}",
                "provincia": "PROVINCIA_EJEMPLO",
                "codigo_municipio": estacion_id,
                "codigo_provincia": estacion_id[:2] if len(estacion_id) >= 2 else "00"
            },
            "coordenadas": {
                "latitud": 40.31138889 + random.uniform(-0.1, 0.1),
                "longitud": -0.28055556 + random.uniform(-0.1, 0.1),
                "altitud_metros": 536.0,
                "sistema_referencia": "ETRS89",
                "precision_ubicacion": "ALTA",
                "geo_hash": f"hash_{estacion_id}"
            },
            "contexto_geografico": {
                "tipo_zona": "RURAL",
                "densidad_poblacion": "BAJA",
                "clasificacion_entorno": "ZONA_NO_URBANA",
                "ine_codigo": estacion_id
            }
        }
    
    def _extraer_caracteristicas(self, html: str) -> Dict:
        """Simula extracci√≥n de caracter√≠sticas de la estaci√≥n"""
        operadores = ["TELEFONICA MOVILES ESPA√ëA, S.A.U.", "VODAFONE ESPA√ëA, S.A.U.", "ORANGE ESPA√ëA, S.A.U."]
        tecnologias = [["2G", "3G", "4G"], ["4G", "5G"], ["3G", "4G"]]
        
        return {
            "titular_principal": random.choice(operadores),
            "operadores_activos": [
                {
                    "nombre": op,
                    "porcentaje_antenas": random.randint(30, 70),
                    "tecnologias": tech,
                    "cantidad_antenas": random.randint(1, 3),
                    "codigo_operador": op.split()[0][:3].upper()
                }
                for op, tech in zip(random.sample(operadores, random.randint(1, 3)), 
                                  random.sample(tecnologias, random.randint(1, 3)))
            ],
            "clasificacion": {
                "tipo_estacion": random.choice(["MEDIA_CAPACIDAD", "ALTA_CAPACIDAD", "BAJA_CAPACIDAD"]),
                "multioperador": True,
                "total_antenas": random.randint(2, 6),
                "total_operadores": random.randint(1, 3),
                "categoria_cnmc": random.choice(["A", "B", "C"])
            }
        }
    
    def _extraer_infraestructura(self, html: str) -> Dict:
        """Simula extracci√≥n de infraestructura tecnol√≥gica"""
        return {
            "antenas_activas": [
                {
                    "id_referencia": f"CSCS-{random.randint(1000000, 3000000)}",
                    "operador": random.choice(["TELEFONICA", "VODAFONE", "ORANGE"]),
                    "banda_frecuencia": {
                        "rango_mhz": "935.10 - 949.90",
                        "frecuencia_central_mhz": 942.5,
                        "ancho_banda_mhz": 14.8,
                        "banda_itu": "900 MHz",
                        "tipo_banda": "LOW_BAND",
                        "banda_3gpp": "B8"
                    },
                    "tecnologia": "2G/3G",
                    "estado": "ACTIVA",
                    "fecha_instalacion": f"201{random.randint(8,9)}-{random.randint(1,12):02d}-{random.randint(1,28):02d}"
                }
            ],
            "resumen_tecnologico": {
                "tecnologias_activas": ["2G", "3G", "4G"],
                "banda_mas_baja_mhz": 796.0,
                "banda_mas_alta_mhz": 3435.0,
                "rango_total_mhz": 2639.0,
                "capacidad_total_mhz": 74.8,
                "indice_diversidad_banda": 0.72,
                "bandas_operativas": ["800 MHz", "900 MHz", "3.5 GHz"]
            }
        }
    
    def _extraer_mediciones(self, html: str) -> Dict:
        """Simula extracci√≥n de mediciones de emisiones"""
        return {
            "puntos_medicion": [
                {
                    "id_punto": "M001",
                    "distancia_metros": 10.0,
                    "valor_medido_uw_cm2": 0.00215,
                    "fecha_medicion": "2023-06-15",
                    "calidad_medicion": "ALTA",
                    "instrumento": "NARDA_EPM-600",
                    "incertidumbre_medicion": 0.0001
                }
            ],
            "analisis_estadistico": {
                "resumen": {
                    "valor_maximo_uw_cm2": 0.00215,
                    "valor_minimo_uw_cm2": 0.00027,
                    "valor_medio_uw_cm2": 0.00112,
                    "desviacion_estandar_uw_cm2": 0.00094,
                    "total_mediciones_validas": 3,
                    "coeficiente_variacion": 83.9
                }
            }
        }
    
    def _evaluar_riesgos(self, html: str) -> Dict:
        """Simula evaluaci√≥n de riesgos para la salud"""
        return {
            "niveles_referencia": {
                "limite_legal_uw_cm2": 450.0,
                "recomendacion_oms_uw_cm2": 100.0,
                "estandar_internacional": "ICNIRP_2020",
                "normativa_espanola": "RD_299/2016"
            },
            "indicadores_cumplimiento": {
                "maximo_porcentaje_limite": 0.478,
                "factor_seguridad_minimo": 209.3,
                "cumplimiento_legal": "CUMPLE",
                "margen_seguridad": "MUY_ALTO",
                "clase_emision": "CLASE_A"
            }
        }
    
    def _analizar_cobertura(self, html: str) -> Dict:
        """Simula an√°lisis de cobertura"""
        return {
            "calidad_general": "EXCELENTE",
            "tecnologias_disponibles": {"2g": True, "3g": True, "4g": True, "5g": False},
            "indices_calidad": {
                "indice_diversidad_tecnologica": 0.85,
                "indice_penetracion": 0.75,
                "indice_capacidad": 0.68,
                "indice_conectividad": 0.82
            }
        }
    
    def _analizar_impacto(self, html: str) -> Dict:
        """Simula an√°lisis de impacto territorial"""
        return {
            "poblacion_servida_estimada": random.randint(500, 5000),
            "area_cobertura_km2": random.uniform(10.0, 100.0),
            "tipo_servicio": "RURAL_FIJO_MOVIL",
            "municipios_servidos": [f"MUNICIPIO_{random.randint(1,10)}"]
        }
    
    def _obtener_estado_actualizacion(self, html: str) -> Dict:
        """Simula obtenci√≥n del estado de actualizaci√≥n"""
        return {
            "ultima_actualizacion": "2024-01-15",
            "proxima_revision": "2024-07-15",
            "estado_operativo": "ACTIVA",
            "confiabilidad_datos": "ALTA",
            "frecuencia_actualizacion": "SEMESTRAL"
        }
    
    def _generar_metadata_scraping(self, url: str) -> Dict:
        """Genera metadatos del scraping"""
        return {
            "url_scraped": url,
            "status_code": 200,
            "response_time_ms": random.randint(200, 800),
            "campos_extraidos": 45,
            "campos_calculados": 22,
            "timestamp_fin": time.strftime('%Y-%m-%dT%H:%M:%S')
        }
    
    async def procesar_batch(self, urls_batch: List[str]):
        """Procesa un batch de URLs concurrentemente"""
        if not self.activo:
            return []
        
        tasks = [self.procesar_url_con_delay(url) for url in urls_batch]
        resultados = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrar resultados v√°lidos
        resultados_validos = [r for r in resultados if r is not None and not isinstance(r, Exception)]
        
        # Actualizar estad√≠sticas
        self.stats['urls_procesadas'] += len(urls_batch)
        self.stats['urls_procesadas_list'].extend(urls_batch)
        
        # Mostrar progreso
        if self.stats['urls_procesadas'] % self.config.progress_update_interval == 0:
            self.mostrar_progreso()
        
        return resultados_validos
    
    def mostrar_progreso(self):
        """Muestra el progreso actual"""
        tiempo_transcurrido = time.time() - self.stats['inicio_tiempo']
        urls_por_segundo = self.stats['urls_procesadas'] / tiempo_transcurrido if tiempo_transcurrido > 0 else 0
        
        print(f"\nüìä PROGRESO: {self.stats['urls_procesadas']} URLs procesadas")
        print(f"‚úÖ √âxitos: {self.stats['urls_exitosas']} | ‚ùå Fallos: {self.stats['urls_fallidas']}")
        print(f"üè≠ Emplazamientos v√°lidos: {self.stats['emplazamientos_validos']}")
        print(f"‚ö° Velocidad: {urls_por_segundo:.2f} URLs/segundo")
        print(f"‚è±Ô∏è  Tiempo: {tiempo_transcurrido/60:.1f} minutos")
    
    async def ejecutar_scraping(self, urls: List[str]):
        """Ejecuta el scraping principal"""
        await self._configure_session()
        
        try:
            lote_id = 1
            for i in range(0, len(urls), self.config.batch_size):
                if not self.activo:
                    break
                    
                batch = urls[i:i + self.config.batch_size]
                resultados = await self.procesar_batch(batch)
                
                # Guardar resultados del lote
                if resultados:
                    self.guardar_resultados_lote(resultados, lote_id)
                    lote_id += 1
                
                # Checkpoint cada X batches
                if (i // self.config.batch_size) % self.config.checkpoint_interval == 0:
                    self.guardar_checkpoint()
                    
        finally:
            await self.session.close()
    
    def guardar_resultados_lote(self, datos_lote: List[Dict], lote_id: int):
        """Guarda un lote de resultados"""
        try:
            archivo_salida = Path('data/resultados') / f"centros_lote_{lote_id:04d}.json"
            
            with open(archivo_salida, 'w', encoding='utf-8') as f:
                json.dump({
                    "metadata": {
                        "fecha_generacion": time.strftime('%Y-%m-%dT%H:%M:%S'),
                        "total_estaciones": len(datos_lote),
                        "lote_id": lote_id
                    },
                    "estaciones": datos_lote
                }, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üíæ Lote {lote_id} guardado: {len(datos_lote)} estaciones")
            
        except Exception as e:
            self.logger.error(f"Error guardando lote {lote_id}: {str(e)}")
    
    def guardar_checkpoint(self):
        """Guarda un checkpoint del estado actual"""
        try:
            checkpoint_data = {
                'stats': self.stats,
                'timestamp': time.time(),
                'urls_procesadas': self.stats['urls_procesadas'],
                'urls_procesadas_list': self.stats['urls_procesadas_list']
            }
            
            checkpoint_file = f"data/checkpoints/checkpoint_{int(time.time())}.json"
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üíæ Checkpoint guardado: {checkpoint_file}")
            
        except Exception as e:
            self.logger.error(f"Error guardando checkpoint: {str(e)}")
    
    def parada_elegante(self):
        """Realiza una parada elegante del scraper"""
        self.logger.info("Iniciando parada elegante...")
        self.activo = False
        self.guardar_checkpoint()
